Text link,Student summary,ChatGPT summary,Realness predictions
http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics1.pdf,"This article mainly discusses the ethics of sharing basic data and what statisticians should do in an appropriate way. The author shows his views that morality is actually a simulating ambiguity. Based on the article, there are only shows the author's perspectives. The authors give an example of a biologist who published a paper on cancer and low radiation, but he didn't publish the original data. There are a large number of statisticians who try to analyze the data and find the standard error. Statisticians waste a lot of time trying to find out there are not accurate the cancer result because the data is based on chicken brains. The author believes that sharing data is a way to develop technology and increase analysis efficiency. The good habit also can reduce the time for other researchers on the same program. 

 ",The chat GPT can't use now. ChatGPT is at capacity right now.,"43.84%,may be very low?"
http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics3.pdf,,,
https://explorable.com/ethics-in-statistics,"Ethics is an important component of statistical analysis. Sometimes we may miss or have fewer ethical considerations when finishing a report. Ethics problems can happen at any stage of statistical analysis. For example, you may not have permission to use the data sources to produce some subjective results in your report. Also, when you try to give out some surveys to collect some data, you may include some misleading questions, which will result in some bias in your results. In the report, statisticians should be objective in their findings because sometimes they don't show the cons but only include pros, which may mislead the audience. ",,"72.86,"
https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2018.01193.x,"The paper ""Ethics in statistical practice and communication: Five recommendations"" by Andrew Gelman provides a guideline for statisticians to conduct communication, practice appropriately, and follow ethics. The five recommendations are as follows:

1. Statisticians should understand and obey the ethical guidelines in statistical research. (For example, don't do cherry-picking and p-hacking)

2. Statisticians should acknowledge the limitation of their clients or collaborators without reservation.

3. Statisticians should promote honesty and transparency in their research.

4. It is also possible for statisticians to make errors and mistakes; it is acceptable to admit and revise. Also, you can actively criticize others and your work. 

5. There are unavoidable limitations to statistics.","The paper ""Ethics in statistical practice and communication: Five recommendations"" by Andrew Gelman offers a set of guidelines for statisticians to ensure ethical conduct in their practice and communication. The five recommendations are as follows:

*

Statisticians should be familiar with and adhere to professional ethical guidelines in statistical research, avoiding practices such as cherry-picking and p-hacking.

*

Statisticians should clearly communicate the limitations and uncertainties of their work to their clients or collaborators.

*

Statisticians should strive for honesty and transparency in their research.

*

Statisticians should be willing to admit and correct mistakes, and actively critique their own work and the work of others.

*

Statisticians should acknowledge and work within the inherent limitations of statistics.","90.07,86.15"
https://www.tandfonline.com/doi/full/10.1080/09332480.2014.890872?casa_token=wdts7OvucVsAAAAA%3AjbbZbRkHWECo_QkWO2r_OSawVfZsHzwUFK-niC07IQesxwq2K1-_gukSwYtPUOh8jAuzEqUrTZg03Q,"Achieving statistical significance and getting the results published in a scientific journal after going through the peer-review processes seem to indicate the credibility of the scientific claim. However, statistical significance is not as hard to obtain in the real world as it was hypothesized. Meanwhile, the peer-review processes cannot prevent false results from publishing because there is a wide range of journals and there is a preference for the novelty of results. Sometimes, the validity of the general research hypothesis is separated from the empirical findings. Replication and pre-registered designs are two common practices for verifying claims. Researchers also need to embrace the existence of variation and accompany their results with correct details, honest appraisals, transparent processes, and less yes/no thinking. They also should not leave the burden of the work to further studies. ",,"99.41,"
https://doi.org/10.1007/s12525-018-0296-3,"Consumer data collection and analysis are becoming increasingly important for data-driven business decisions and improvement in customer experience. It is common for companies to consider data protection measures as obstacles to profitability. Yet, data privacy scandals of enterprises such as Home Depot and Apple have proven that failing to protect personal data could cause economic damage and competitive disadvantage. This article hence examines the effect of privacy protection measures in retail. By analyzing data from online surveys, the authors concluded that an appropriate level of privacy protection boosts consumer satisfaction and becomes a competitive advantage for the company. In other words, consumers tend to prefer companies that put efforts into protecting consumer data. 

In addition, the authors proposed a systematic framework for implementing privacy measures that address consumers' major privacy concerns (namely, data collection, data combination, secondary usage, errors, improper access and automated decision-making). The framework emphasizes informed consent, anonymization, restraint and empowerment of consumers at all stages of data collection and analysis. The authors argue that the design and implementation of privacy measures should take into account the consumers' concerns and perspectives, and should exceed the legal requirements. ","As data-driven decision making and customer experience improvement become more critical for businesses, the collection and analysis of consumer data is becoming increasingly important. However, some companies view data protection measures as a hindrance to profitability. However, the data privacy scandals of companies such as Home Depot and Apple have shown that failing to protect personal data can result in financial loss and a competitive disadvantage. This article examines the impact of privacy protection measures in retail and, through analysis of online survey data, concludes that an appropriate level of privacy protection enhances consumer satisfaction and serves as a competitive advantage for the company. Essentially, consumers tend to prefer companies that make an effort to protect their data.

The authors also propose a systematic framework for implementing privacy measures that address consumers' main privacy concerns, including data collection, data combination, secondary usage, errors, improper access, and automated decision-making. The framework emphasizes informed consent, anonymization, restraint, and consumer empowerment throughout the data collection and analysis process. The authors argue that privacy measures should be designed and implemented with consideration for consumers' concerns and perspectives and should exceed legal requirements.","99.98,99.95"
"http://www.stat.columbia.edu/~gelman/research/published/ethics20_revised_3.pdf

 ","Author writes about the significance of ethical considerations in statistical practise and communication in an article titled ""Ethics in Statistical Practice and Communication: Five Recommendations."" Beginning with the fact that statistics is frequently used to make significant decisions and shape public opinion, he emphasises the importance of practitioners and communicators of statistics giving ethical considerations top priority in their work. 

The first piece of advice the author offers is to be open and honest about the information, procedures, and presumptions utilised in any statistical study. This entails outlining any constraints or uncertainties in the data as well as any possible bias sources. Additionally, he counsels professionals to be open and honest about any potential conflicts of interest and their degree of knowledge.

His second piece of advice is to utilise statistics fairly and objectively, refraining from selecting just relevant data or shaping the results to fit a predetermined goal. Rather than employing technical jargon or making too general or deceptive statements, he advises statisticians and communicators to attempt to provide information in a way that is clearly understandable and accessible to a wide range of people.

The third piece of advice is to be conscious of the possible outcomes of one's work and to take into account any potential social implications. The author, Gelman counsels statisticians to examine the potential harm that could arise from abuse or misinterpretation and to think carefully about the potential uses of their work.

His fourth piece of advice is to actively look for other viewpoints and viewpoints from people who might be impacted by the research. By doing so, the research will be inclusive and it will be less likely that incorrect assumptions are made.

Finally, Gelman counsels statisticians to cooperate and to be receptive to criticism from both peers and the general public. He contends that by doing this, it will be feasible to guarantee both the greatest level of research quality and its usage in the most moral and responsible manner.

 ","In his article ""Ethics in Statistical Practice and Communication: Five Recommendations,"" the author stresses the importance of considering ethical issues when using statistics to make important decisions and influence public opinion. He offers five pieces of advice for statisticians and communicators to follow: be transparent and honest about the information, procedures, and biases used in a study, use statistics objectively and avoid shaping results to fit a predetermined goal, consider the possible consequences of the research and its impact on society, actively seek out diverse perspectives and be open to criticism from peers and the public. The author, Gelman, emphasizes the importance of honesty, fairness, and awareness of potential effects in ethical statistical practice and communication, and encourages collaboration and inclusivity in research.

 ","98.68,90.60"
http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics10.pdf,"The article written by Andrew Gelman talked about a common phenomenon between subprime bonds and the scientific process. He first introduced the overheated finance industry, which was expanding quickly with highly understated risk. However, both professionals and consumers continued to invest and operate in the industry with the extremely optimal belief. Similarly, in the scientific field, it also experience explosive growth with huge public researcher papers analyzing on the combination of different quality of data. Although the result might be correct due to the huge amount of data, these suspicious data points increase people’s worries regarding the validity and dependability of published results.

In order to reduce such worries, statisticians need to be honest about the associated uncertainty and variation, and they could follow the list from the lessons of the financial crisis to improve: 
1.    Get the details right. 2. Make the honest appraisals. 3.Don’t pass along the risk. 4.Keep the ratings agencies honest and transparent. 5. Be careful about categorical, yes/no reasoning. ","Andrew Gelman wrote an article discussing the similarities between the subprime bond market and the scientific process. He highlighted the rapid expansion of the finance industry, despite understated risk, and the continued investment and participation of both professionals and consumers. Similarly, the scientific field has experienced significant growth, with an abundance of research papers analyzing various data. However, the presence of questionable data points raises concerns about the validity and reliability of published results. To address these concerns, statisticians should be transparent about uncertainty and variation and follow the lessons from the financial crisis, such as ensuring accuracy, making honest evaluations, not passing on risk, maintaining transparency and accountability with ratings agencies, and being cautious about binary reasoning.","59.44,36.25"
"https://www.ibm.com/topics/ai-ethics

 

 ","This article covers a range of AI ethics-related subjects. AI ethics is a set of rules that govern the development of artificial intelligence based on moral principles to prevent any violations of human rights. As technology develops, it is increasingly important to take AI ethics into account when creating new algorithms and products. Respect for people, beneficence, and fairness are the three guiding principles for designing experiments and algorithms. The main issues with AI today span a number of areas. People are initially concerned about the technological singularity problem, which is a theory that AI will eventually become uncontrollably. Second, AI has a significant impact on the labour market, by shifting the demand for roles or displacing some employment. Third, there are significant privacy and data security concerns. Fourth, AI raised the question of how we could safeguard against bias and discrimination in the algorithm. Lastly, there is no real enforcement mechanism currently to ensure ethical AI practice.

To address the aforementioned concerns, people have worked on assembling frameworks and achieved some consensus. First, there should be governance within the company structure to manage ethical AI. Next, we should ensure explainability of the models and build transparency to help people understand how does a specific model arrive at a certain decision point. 

At last, the article also introduces some organizations that promote ethical conduct of AI, such as AlgorithmWatch, AI Now Institute, DAPRA, CHAI, and NASCAI.

 

 

 ","This article delves into various aspects of AI ethics, which refers to the set of rules and principles governing the development of artificial intelligence in order to prevent any violations of human rights. As technology advances, it becomes increasingly important to take AI ethics into consideration when creating new algorithms and products. The three main principles guiding the design of experiments and algorithms are respect for individuals, beneficence, and fairness. Currently, there are several key issues with AI, such as the potential for AI to become uncontrollable, the impact of AI on the job market, concerns about data privacy and security, potential biases and discrimination in algorithms, and the lack of enforcement of ethical AI practices. To address these concerns, frameworks have been developed and some consensus has been reached. For example, there should be internal governance within companies to manage ethical AI, models should be made explainable and transparent, and organizations that promote ethical conduct in AI include AlgorithmWatch, AI Now Institute, DAPRA, CHAI, and NASCAI.

 

 ","99.98,99.97"
"https://jme.bmj.com/content/medethics/19/4/219.full.pdf

or 

https://jme.bmj.com/content/19/4/219.short

 

 ","There are some arguments in the article: 

* In statistics, there are two different approaches, Bayesian and frequentist approaches. The Bayesian approach allows us to use our prior knowledge of the relevant topics to propose prior distribution on the parameters, whereas the frequentist does not allow us to do so. So which approach should we use?

* There are Individual and collective ethics. Individual ethics cares about each individual, whereas collective ethics cares about the community, a group of people. So which one should we care about more?

* In the medical field, should we care more about current patients or future patients?

The author's suggestion:

The author used the clinical trial as an example and explained whether we should use Bayesian or frequentist statistics depending on the content. We might want to use the Bayesian approach in early-phase trials and for lethal diseases. However, we might want to use the frequentist approach for later phase trials and for less serious diseases.","The author suggests that the choice between Bayesian and frequentist approaches in statistics should be based on the specific context and content of the study. For example, in early-phase trials and for lethal diseases, a Bayesian approach may be more appropriate because it allows for the use of prior knowledge to propose prior distributions on the parameters. However, for later phase trials and for less serious diseases, a frequentist approach may be more appropriate.

The author also suggests that the choice between individual and collective ethics should also be based on the specific context and content of the situation.

In the medical field, the author suggests that the choice between caring more about current patients or future patients should also be based on the specific context and content of the situation.","99.97,1.89"
"https://online.stat.psu.edu/statprogram/ethics

 ","In the article ""Ethics and Statistics"" from the department of statistics in Penn State Eberly College of Science, the author suggests ethics in various perspectives related to statistics. To begin with, the definition of ethics is provided by the author saying that ethics is a set of ethically acceptable guidelines that everyone wants to conform to. Moral literacy is also introduced and all statistician should be aware on that. The author also introduces ethical sensitivity that we could know the categories of harm, fairness, loyalty, respect, and sanctity. Ethical thinking is also important, we should think to ourselves on for example, whether we are proud our ashmed on a specific action. After that, we need to imagine a situation of this ethical problem and see how we can reflect on it. Last but not least, the author explains seven steps we should take for ethical decision making.We should first state the issue, verify all facts, identify the essential components, create a list of potential options, test those options, repeat earlier processes as necessary, and finally review every step.",,"84.59,"
http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics3.pdf,"Instead of being an independent researcher, some physicians choose to become contract researchers. The author was worried since such researchers do not provide research insights but simply sign off the contracts which endorses certain research results produced by the enterprises that employ them. Since there exists no common statistical method to handle all types of data, the author provides three ideas for statistician to contribute for elimination of such issue:

* move towards openness: evaluate all results; do not simply focus on the statistically significant ones.

* Decision analysis: do not stop at reporting confidence intervals

* Question the assumptions that we tends to use in multiple conventional analysis

The author summarises that as statisticians, we should be open and quantitative about the tradeoffs.","Some physicians opt to work as contract researchers instead of being independent. However, the author expresses concern that these researchers do not conduct their own research, but simply approve the results produced by the companies they work for. To address this issue, the author suggests three ways that statisticians can contribute to solving it:

* Embrace openness by evaluating all results, not just those that are statistically significant.

* Use decision analysis to go beyond reporting confidence intervals.

* Challenge the assumptions commonly used in traditional analysis.

The author concludes that statisticians should be transparent and quantitative about the tradeoffs they make.","99.82,52.23"
https://www.analyticsvidhya.com/blog/2022/02/ethics-in-data-science-and-proper-privacy-and-usage-of-data/ ,"Ethics in data science is a focused issue in people's daily life. As humans, we must have a natural sense of what is right and what is wrong. Data science plays a significant role in many fields; thus, ethical considerations are vital for personal privacy and social impact. Furthermore, everyone's personal information is exposed to the applications they use since people live in a digital world. Anyone working with data must be concerned about data science ethics, understand the fundamentals, and report any data theft, unethical data collection, storage, use, etc. Data protection must be maintained at any time. A data scientist must contact a client first and then make decisions. Data collection and analysis must be done with good intentions. Obtaining others' data without their consent is illegal and unethical. Therefore, collecting anyone's data requires approval. Lastly, transparency should be used when acquiring data. The article also includes three real-life examples, which are regarding a case of unethical behavior with data, a case of data theft, and an example of how data is used for a good purpose.",,"99.83,"
https://statmodeling.stat.columbia.edu/2023/01/25/water-treatment-and-child-mortality-a-meta-analysis-and-cost-effectiveness-analysis/,"1. RCTs have shown that the improved water quality can significantly reduce the child mortality rate

2. leading cause of death in children is diarrhea, caused by waterborned diseases

3. chlorinating is a cheap way to cleanse water yet many countries are able to do it

4. it's extremely cost-effective to avert child mortality","The author is a co-author of a meta-analysis of improving water quality in low and middle income countries. The meta-analysis estimates that reducing waterborne diseases by improving water quality reduces the odds of child mortality by 30% based on 15 RCTs. The author points out that this would have very large real-world implications, but there are statistical considerations such as power, publication bias, etc. The author also ties the discussion to effective altruism and finding cost-effective interventions. The author notes that clean water is an important topic as diarrhea caused by waterborne diseases is one of the leading causes of death in children globally, and many people still don't have access to clean water. The author also mentions that the cost-effectiveness of this is on par with the most cost-effective child health interventions such as vaccinations.","99.54%,91.90%"
https://towardsdatascience.com/lessons-from-how-to-lie-with-statistics-57060c0d2f19,"This article discussed the right ways to present and to interpret statistics. The author started by introducing the book ""How to Lie With Statistics"" by Darell Huff and stated that the book is relevant today because small tables, graphs, or a single number are still the most effective techniques. From the book and their experiences, the author gave suggestions to both producers and consumers of statistics whereby the suggestions were sectioned into 10 aspects:

1. be careful when viewing correlations due to potential confounding factors

2. relationships are usually only valid within a limited region

3. axes can easily mislead

4. extremities may occur just due to too small sample sizes

5. more numbers describe a data set better

6. ""average"" has alternative definitions and should be checked

7. a statistic may only be meaningful in comparisons

8. there is always bias in sample selection and it matters

9. we should not assume that claims from authorities are true

10. in general, be skeptical to any single statistic",,"99.89,"
https://globalnews.ca/news/9432503/chatgpt-exams-passing-mba-medical-licence-bar/,"ChatGPT is capable of passing the U.S. Medical Licensing Exam and might even acquire an MBA from an Ivy League business school, according to two distinct study publications. However, the accuracy of ChatGPT on the Multistate Bar Examination, a multiple-choice exam that is part of the U.S. legal license process, was about 50 percent, according to another research. In the near future, ChatGPT will likely be able to pass the U.S. bar exam, as stated by the researchers. ChatGPT has already started to transform our educational philosophy. But, no one can determine with certainty how AI will affect the future of employment. It is certain that humans will have much less market in intelligence and creativity.","According to studies, ChatGPT has the capability to pass the U.S. Medical Licensing Exam and potentially even earn an MBA from an Ivy League business school. However, its accuracy on the Multistate Bar Examination, a multiple-choice exam required for legal licensure in the U.S., was found to be around 50%. Researchers believe that in the future, ChatGPT will be able to pass the bar exam. The impact of AI on employment and the job market is uncertain, but it is likely that AI will significantly reduce the need for human intelligence and creativity in certain fields.","6.87,93.13"
"https://online.hbs.edu/blog/post/types-of-statistical-bias

 

*****************************************************
5 TYPES OF STATISTICAL BIAS TO AVOID IN YOUR ANALYSES
*****************************************************

 

 ","It is important to note that statistics is as much of an art as it is a science with two different statisticians given the same data set and goal can still produce differing results. A large part of this has to due with the fact that a lot of decisions have to be made in analysis and these decisions open the door to many different types of biases which can alter the accuracy and generalizability of results. Following are some types of commonly found biases :

Sampling Bias

 When choosing random sample to collect data from it is important to ensure the sampling group is representative of the population. In most cases it isn't feasible to have a sample population that contained an equal likelihood for all the population to be chosen. This can lead to an unequal representation from all the demographics of the population leading to inaccurate results and a loss of generalizability.  

Bias in Assignment

In many cases studies require multiple distinct groups to be created to be studied. In this case careful consideration has to be put into how these groups are created as bias can be introduced though these groups containing unintended characteristics and features that might impact results. It is important to properly randomize and diversify your groups to avoid these biases. 

Self-Serving Bias and Experimenter Expectations

It is important to understand that statisticians are people as well. In an experiment the researchers expectations and the desire to attain a specific results might introduce bias in an experiment. This can be seen through p hacking and cherry picking data to fit a certain result. In statistics it is important that we follow guidelines to ensure research is reproducible and accurate. 

 ",,"99.98,NA"
https://www.youtube.com/watch?v=het9HFqo1TQ&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=3&ab_channel=StanfordOnline,"The article is basically listing all the steps that are required to write a good statistical report. The overall steps can be breifly divided into 9 parts which include ""Introduction"", ""The goal of the report"", ""Understanding the intended readers"", ""The major section of the report"", ""Style"", ""Tables and Figures"", ""Writing the First Draft"", ""Rewriting the report"", ""References"". Each of them takes a significant part in a report writing and besides all of these, the writing skills it-self is also as important. There is a whole sections introduce the importance of having good writing skills.","This article outlines the nine key steps to writing a successful statistical report. These steps include: Introduction, The Goal of the Report, Understanding the Intended Readers, The Major Sections of the Report, Style, Tables and Figures, Writing the First Draft, Rewriting the Report, and References. Each of these steps plays an integral part in producing an effective report, but it is also important to possess strong writing skills. A whole section of the article is dedicated to the importance of having good writing skills.","99.98,75.43"
https://slate.com/technology/2018/10/brian-wansink-p-hacking.html,"The article discusses an event that a well-known researcher has been caught for statistical misconducts including p-hacking. While most people focus on the wrong action of manipulating the data itself, the author Andrew Gelman calls for attention to the natural difficulties of many statistical researches. That is, people should not expect a lot of studies will have significant results, and fall into the trap of trying to get important findings no matter what. As statistics researchers, while need to be conscious and careful of not doing morally incorrect things like p-hacking, they also need to focus on designing good statistical models/approaches and be patient even if there aren't great findings.","The article addresses the issue of a prominent researcher being accused of committing statistical misconduct, specifically p-hacking. Andrew Gelman emphasizes that instead of just condemning the misconduct, attention should also be given to the inherent challenges of statistical research. It is unrealistic to expect every study to yield significant results, and researchers must avoid the temptation to manipulate data in pursuit of important findings. As statisticians, it is important to maintain ethical standards and avoid misconduct, but it is also crucial to focus on designing sound statistical methods and to be patient in the absence of groundbreaking results.","99.21,97.95"
"https://www.nature.com/articles/d41586-022-00025-6

 ","This article discussed the importance of reviewing the reliability of data from medical studies and proposed some guidelines of assessing the quality of data for reviewers. In general, it is hard to evaluate the appropriateness of a literature because there is no universal tellable sign of fraud. To ensure the reliability of results from clinical trials, reviewers can check registration, consult the corresponding database, check the plausibility and consistency of claims. Contacting the authors is the most direct and effective way to understand the data. If the author is not reachable, then we can seek help from the journal. It is the responsibility for journals and publishers to check all the materials and detect anomalies in time. ","The article discusses the issue of fake clinical data in the medical research field, and suggests that a collaborative effort from various stakeholders, including researchers, journals, and funding agencies, is needed to effectively address the problem. The article highlights the potential consequences of fake data, such as wasted resources and harm to patients, and notes that current methods for detecting and preventing fake data are not sufficient. The article suggests that a multi-faceted approach, including better training for researchers, stricter guidelines for data sharing and reporting, and increased transparency and accountability, may be necessary to effectively combat the problem of fake clinical data.","98.69,5.92"
"https://youtube.com/watch?v=mtLPd2u4DiA&feature=shares

 ","* Good research not only should be empirical, reproducible, and valid but also should be ethical. Ethics is the moral principle that guides our judgment of right and wrong.

* Different organizations usually have their own code of ethics. Ethical standards will vary by region, discipline, or institution, but there will be common themes that determine what constitutes research ethics. 

* In order to achieve ethical research, there will be committees to decide what is ethical and what is unethical, these committees are made up of people from different backgrounds so they will provide different ethical perspectives. Also, there are different types of committees depending on the person or thing involved in the research. For example, in academia and industry, ethics committees review research proposals, oversee their implementation and follow up on their results, and if they determine that the research is unethical, the committee has the authority to approve, reject, stop or modify the research.

* Then, the video provides some examples of unethical research and points out that unethical research may occur out of a desire for recognition and fame, it may be done to ensure that a project is completed as quickly as possible, it may be done to get a jump on being the first to publish a discovery, or it may be done to climb the academic ladder.

* At the same time, bias and prejudice can lead to ethical violations. Unethical research may have consequences such as public distrust of science, or harm to individuals. That is why it is necessary to take measures to prevent unethical research from happening in the future. 

* Finally, ethics training is important not only to prepare current and future researchers to work in the disciplines they want to work in but also to make new discoveries when researchers have knowledge of ethics.","In summary, ethical research is important in ensuring that studies are conducted in a responsible and just manner. Ethics committees review research proposals and oversee their implementation to ensure that they align with ethical standards. Unethical research can occur due to a desire for recognition or to meet deadlines, and can lead to harm to individuals and a loss of trust in science. Therefore, it is crucial to provide ethics training to researchers to promote ethical conduct in research.","99.92,56.82"
"https://librarysearch.library.utoronto.ca/permalink/01UTORONTO_INST/fedca1/cdi_gale_businessinsightsgauss_A688710549

If the link isn't working, please search for ""Predictably unequal? The effects of machine learning on credit markets"".","* With the innovation in technologies, machine learning models have gained wider applications. One of their applications in a finance context is to predict the probability of default (borrowers failing to repay loans). 

* This paper focuses on whether the more advanced ML models are ""predictably unequal"", e.g. the model has different predictive outcomes for minorities versus the White. The authors investigated two main mechanisms driving this prediction difference 1) model flexibility and 2) triangulation between observable permissible variables and unobserved restricted variables

* The authors illustrate this phenomenon with U.S. mortgage data. By comparing predictions from logistic regression with those from the random forest, authors found that Black and Hispanic borrowers are penalized by the usage of the ML model - they are predicted to have a higher probability of default.",N/A,"99.98,N/A"
http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics2.pdf,"This article mainly focuses on explaining the disconnect between the statistical methods taught in academic statistics classes and the methods used in practice. Based on the authors' arguments, we find that the statistical methods we learn in courses are not appropriately used in real-world situations because real-world data analysis is more complex. It also means that although statisticians often advocate open data and open methods in their research, these principles are not consistently applied in teaching. This results in the majority of students graduating from statistics disciplines not being better equipped to perform real-life data analysis. Therefore, this article argues that statistical practice should be more research-based and more practically focused, and use open data and open methods.","The article discusses the discrepancy between the statistical techniques taught in academia and those utilized in the field. The authors assert that the methods learned in statistics classes are not sufficient for handling real-world data analysis due to its complexity. Additionally, they note that while statisticians often promote transparency in data and methods in their research, these principles are not consistently incorporated into teaching. This leads to students graduating without the necessary skills for practical data analysis. The article argues for a shift towards research-based, practical training that emphasizes open data and open methods.","97.89,7.32"
http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics4.pdf,"It is inevitable that statisticians will work for a variety of organizations. Upholding the code of ethics expected of a statistician is imperative when working for a controversial organization. All while speaking honestly and disclosing our financial interests from controversial organizations. It is important to note that sometimes speaking honestly about statistical findings may involve broader ethical issues and thus leave the field of statistical ethics. In this case, we need a deeper discussion on ethics. The same conflict may appear in other fields, such as journalists, doctors and other professions. It is better to abide by professional ethics than to violate professional ethics to obtain broader ethical support.","Statisticians often work for a diverse range of organizations, and it is crucial for them to uphold the code of ethics expected of their profession, particularly when working for controversial organizations. This includes being transparent about any financial interests and speaking honestly about statistical findings, even if doing so raises broader ethical issues. In these cases, it is important to have a deeper discussion on ethics, as similar conflicts may arise in other professions such as journalism and medicine. Ultimately, it is more important to adhere to professional ethics than to compromise them in order to gain broader ethical support.","99.56,94.09"
"https://www.tandfonline.com/doi/full/10.1080/09332480.2017.1302720?casa_token=tgzmCX10N2QAAAAA%3AV24HNuHm3IwYrEjtDS2IXhr2rE203KwC-1nzzqfwOA1cpC3abvzJ1vsjZSrGddrnibRl_0iqRTkW

 ","There is a significant replicability crisis in statistics in many top scientific journals. Often, due to confidentiality and author choice, many published articles do not present their data and code. This makes it difficult to verify the accuracy of the study. There is also a risk that some authors may falsify their findings through this loophole. 

Many have suggested that honesty and transparency are the solutions to this crisis, but the author of this article, Andrew Gelman, disagrees, arguing that honesty and transparency are not enough. Data quality and design of study are also important. The honesty and transparency of a published article cannot compensate for the errors made by the researcher in the study, which may have been unintentional. So simply requiring honesty and transparency may lead to people associating these inadvertent statistical errors with dishonesty, and thus not being able to learn from them. 

 ","The field of statistics is facing a major problem with replicability in top scientific journals. Many studies do not share their data and code, making it difficult to verify their accuracy and leaving room for potential falsification of findings. Some suggest that increasing honesty and transparency is the solution, but the author of this article argues that this alone is not enough. Ensuring data quality and good study design are also crucial. Requiring honesty and transparency may lead to confusion between intentional dishonesty and unintentional errors, making it difficult to learn from them.

 ","75.80,97.93"
https://www.amstat.org/your-career/ethical-guidelines-for-statistical-practice,"The article Ethical Guidelines for Statistical Practice contains several principles to help statistical practitioners regulate professional conduct. The Statistical Practice includes the activities like collecting the data, processing and analyzing the data, developing algorithm, etc, while statistical practitioners includes all the people that participate in statistical practice. The principles are summarized as follows:

Principle A: Taking responsibilities to one's own work, being honest with the capabilities and activities, respect others.

Principle B: Honestly communicate the potential impacts in their works.

Principle C: Respecting the interests of stakeholders.

Principle D: Never misusing or condoning the misuse of data.

Principle E: Working ethically with members in multidisciplinary teams.

Principle F: Being responsible to the statistical practitioners and profession. Honestly communicate and engage in the work with other practitioners and the profession.

Principle G: Leaders, supervisors and mentors are responsible for following and promoting these ethical guidelines.

Principle H: Statistical practitioners understands that questions may arise concerning potential misconduct related to statistical, scientific, or professional practice. They may be involved in the investigation.","The Ethical Guidelines for Statistical Practice is a set of principles to guide the conduct of statistical practitioners in their work. These principles include taking responsibility for one's own work and being honest about capabilities and activities, honestly communicating the potential impacts of work, respecting the interests of stakeholders, not misusing or condoning the misuse of data, working ethically with members of multidisciplinary teams, being responsible to other practitioners and the profession, and leaders, supervisors, and mentors being responsible for promoting these guidelines. Additionally, statistical practitioners should be aware that questions about potential misconduct may arise and they may be involved in investigations.","99.98,99.97"
http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics8.pdf,"In Andrew German's article ""It’s Too Hard to Publish Criticisms and Obtain Data for Replication"", German pointed out some of the existing problems within the system of publication that discouraged or prevented scientific corrections to published papers. German listed several examples from his own experience to support his claim. The first example was that a paper by y Laura Hamilton, published on several top journals, such as American Sociological Review, made conclusions based on results estimated inappropriately. The estimation was completed based on samples subjected to selection bias, as Hamilton only included samples negatively impacted by the treatment in estimation. Finding an error was easy, yet it became really difficult when German tried communicating with the author and the journal editors about making corrections. It was difficult to get original data due to privacy reasons, and even after he got data and conducted his own analysis, the journal editor refused to make changes since there was a rule that any request for changes had to be ranked at top 10 percent among other requests in terms of quality. After this example, German continued discussing about other examples when he encountered defensive authors of papers that involve mistakes in estimation. In the end, German emphasized how important it is to make sure the reproducibility and accessibility of the materials used in existing papers, so that science can correct itself successfully over time.",,"99.98,"
"https://medium.com/analytics-vidhya/data-ethics-in-artificial-intelligence-machine-learning-72467b9c70f3

 ","This article summarizes the ways AI and machine learning algorithms could be misused and confused by introducing a few specific examples where unethical elements and biases are present. It also highlights that people should follow the 5Cs principle (Consent, Clarity, Consistency & Trust, Control & Transparency, and Consequences) while collecting data and building algorithms to make the whole system more ethical. 

Besides, the article mentions a checklist of potential questions and concerns data scientists should keep in mind and ask themselves while building algorithms and making decisions. In the end, it is important that the whole machine learning community could create a sense of responsibility among all its participants to avoid ethical or privacy issues. ","It seems ChatGPT is at capacity right now, so I'll just skip the optional question. ","99.51,NA"
https://jezebel.com/what-happens-when-you-tell-the-internet-youre-pregnant-1794398989 ,"This article is about the privacy of consumer data, or lack thereof, with mobile apps, specifically with regards to pregnancy and period tracking apps. The author of the article spoke about her experience downloading many of the mobile apps targeted to pregnant women or women trying to become pregnant to track fertile dates, symptoms, etc. In particular, she speaks about the security and privacy concerns with many of these apps, as these apps often require users to share very sensitive and personal information. Many apps often will pass on their users' information to third-party advertisers or to create targeted ads, or even sign up their users to marketing groups. The author concludes by bringing up the point that in the future there may be a need for laws or regulations to protect people's health information/privacy.","This piece discusses the issue of consumer data privacy in mobile apps, with a focus on pregnancy and period tracking apps. The author recounts their personal experience using such apps and highlights the security and privacy concerns that arise from the sharing of sensitive personal information. They mention that many apps may pass on this information to third-party advertisers or use it to create targeted ads, or sign up users to marketing groups. The author concludes by suggesting that there may be a need for laws or regulations to protect people's health information and privacy in the future.","99.92,91.49"
https://embassy.science/wiki/Theme:6b584d4e-2c9d-4e27-b370-5fbdb983ab46,"What is this about?

* P-value hacking is known as data dredging.

* P-value hacking involves repeatedly testing a dataset until achieving statistical significance in the results.

* For example: Only reporting the outcomes that were achieved by P<0.05.

Why is this important?

* High-quality journals mainly publish statistically significant outcomes, which brings pressure on the researchers who pursue academic prestige and job, thereby leading the p-value hacking.

* P-value hacking has negative impacts on future research.

For whom is this important?

Students/Ph.D. Students/Scientists/Researchers/Supervisors/Postdocs/Journal editors/Junior/researchers/Senior researchers

What are the best practices?

* Cross-validation or out-of-sample testing is a statistical method used to create two sets of data, which could be a way to prevent p-value hacking. One set of data is used to do the statistical analysis. The other set is used to verify them.

* Several journals are now asking for raw data to be published, or they are shifting to a registered reports format.

 ","P-value hacking, also known as data dredging, is a technique in which researchers repeatedly test a dataset to achieve statistical significance in the results. This is often done by only reporting outcomes that reach a certain level of significance, such as P<0.05. This practice is driven by the pressure on researchers to achieve academic prestige and job opportunities, and leads to the publication of false conclusions in high-quality journals. P-value hacking can have negative impacts on future research. To prevent this, cross-validation or out-of-sample testing can be used to create two sets of data, one for statistical analysis and the other for verification. Some journals are also now requiring raw data to be published or shifting to a registered reports format. This is important for students, Ph.D. students, scientists, researchers, supervisors, postdocs, journal editors, junior and senior researchers.","97.42,0.71"
"https://blogs.ams.org/blogonmathblogs/2014/04/10/bad-statistics-ignore/

 ","This article is talking about how to react to the publication of the bad statistics, the research that carries wrong information and may mislead the public. There is a trade-off between criticize the article or ignore it. On one hand, scientists are worried that harsh criticism would discourage new research, in particular for shaming people. On the other hand, not correcting the bad study especially about health may also result in harmful consequence. However, it's also possible that debunking would draws more attention to the original article, which gives opposite effect.

The author proposes to find a balance between the two types of the responses. The level of pointing may depend on the field, the stage of the research, and many other accountable factors. When scientists are writing some skepticisms, hopefully it is persuasive enough and is able to distract from the crappy articles.",N/A,"99.98,N/A"
https://stattrak.amstat.org/2011/07/01/prof-ethics-advice/ ,"The article had two main points, the first of which is always to be a strong advocate with your statistical decisions, and the second is that ethics should be more intertwined with statistics education. The first point was how students or practicing statisticians would find design flaws in experiments or analyses and bring them up with their peers. However, they wouldn't be too outspoken with their opinion and instead be quite reserved. The article argued that practicing statisticians should always be prepared to voice their thoughts when they believe an analysis could be done incorrectly. The second point is quite self-explanatory in that professors shouldn't try to make their statistics courses or the ethics section within the course easy. The author argued that the long-term detriment of not taking ethics seriously could have severe scientific consequences.",The article emphasized the importance of being a vocal advocate for sound statistical decisions and the integration of ethics in statistics education. It urged statisticians to speak up when they suspect a design flaw or incorrect analysis and stressed the need for professors to approach ethics in statistics education with gravity and seriousness to avoid negative scientific consequences.,"95.45,99.67"
"[How do you deal with ethical problems of AI?] (https://medium.datadriveninvestor.com/how-do-you-deal-with-ethical-problems-of-ai-944303da67d3)

In case the link above doesn't work:

https://medium.datadriveninvestor.com/how-do-you-deal-with-ethical-problems-of-ai-944303da67d3","Despite the great benefits that AI may bring to human innovations, the ethical problems in AI, including discrimination and bias, could also lead to unfortunate consequences such as unemployment and unfair treatment in the criminal justice system. Christophe Atten addressed the ethical considerations of artificial intelligence, including transparency, accountability, fairness, privacy, governance, and public engagement. With these important factors in mind, the author also proposed several ways to develop an ethical AI, which involves the collaborative efforts of the developer, users as well as policymakers. Also, by generating more active public discussions of AI ethics, more appropriate guidelines, laws, and other governance mechanisms for AI usage may be established, hopefully leading to a better AI environment with mindful usage, privacy protection, and no discrimination.","Artificial intelligence (AI) has the potential to bring significant benefits to human innovation, but it also poses ethical problems, such as discrimination and bias, which can lead to negative consequences such as unemployment and unfair treatment in the criminal justice system. To address these concerns, Christophe Atten suggests considering transparency, accountability, fairness, privacy, governance, and public engagement in the development of AI. He also proposes that collaboration between developers, users, and policymakers is necessary to create ethical AI. By fostering active discussions about AI ethics, appropriate guidelines and laws can be established to ensure a responsible and inclusive AI environment that prioritizes privacy and avoids discrimination.","99.98,97.75"
https://plato.stanford.edu/entries/ethics-ai/,"In this article, the author Müller talked about some fundamental ethical questions that arose from the application of AI and robotics systems. Below are the main debates that existed in society regarding AI ethics:

1. The underlying logic of how AI and robotics work is machine learning. Computers learn from massive data to extract patterns and then educate themselves to take action. But the data collection process is considered problematic regarding privacy and ethics. AI increased the possibility of data collection and analysis but also increased the risk of data misuse. 

2. The process and result of the AI system might be opaque and biased. Even the programmers may not fully understand how the machine identifies patterns and what patterns the machine chooses to learn. In addition, the quality of the result depends heavily on the quality of the input data. If the input data is highly biased, then AI will reproduce the bias. 

3. Human-Robot interaction might also lead to ethical issues regarding human rights and respect for humanity. The primary goal of 'care robots' is to supplement the caring professions. But the robots might ignore the core of caring for a person is letting him/her feel love and giving them emotional support. Robots can do systematic and repetitive work but they might be weak for personalized work.

4. AI and robotics automation resulted in “job polarisation”, where highly-skilled technical jobs and low-skilled service jobs have remained but jobs between these two polarities are under pressure. Also, whether the development of AI is environmentally friendly is adjustable. 

5. Let AI make appropriate decisions between optimized results and ethical choices might be difficult. ",,"99.95,"
http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics3.pdf,"The column ""Ethics in Medical Trials: Where Does Statistics Fit In?"" written by Andrew Gelman analyzed statisticians' contributions to ethics in medical research through two real-world examples. The first example was about a clinical research involving many pharmaceutical professionals and contract researchers, while the researchers didn't do much work but just reports signing. The doctors seemed to be responsible for most work but didn't work much, only focusing on providing the research results that the company wanted. The second case discussed a cancer drug, Avastin, which was reported to have not much significant positive effect on breast cancer patients. Even though Avastin provided some effectiveness, for personal merit, one national group strongly preferred the approval, which was not ethical. 

From the two cases, as statisticians, what we can help with ethnicity in medical trials is not focusing on only statistically significant information but being open to all significant information. It would also be helpful for us to help the medical professionals deal with less approachable and less controllable data, aiming to make them less willing to turn to be unethical as a form of escapism from those data. In addition, the author pointed out that being quantitative to all valuable information would be essential for statisticians in medical trials. ","The column ""Ethics in Medical Trials: Where Does Statistics Fit In?"" written by Andrew Gelman analyzed statisticians' contributions to ethics in medical research through two real-world examples. The first example was about a clinical research involving many pharmaceutical professionals and contract researchers, while the researchers didn't do much work but just reports signing. The doctors seemed to be responsible for most work but didn't work much, only focusing on providing the research results that the company wanted. The second case discussed a cancer drug, Avastin, which was reported to have not much significant positive effect on breast cancer patients. Even though Avastin provided some effectiveness, for personal merit, one national group strongly preferred the approval, which was not ethical. 

From the two cases, as statisticians, what we can help with ethnicity in medical trials is not focusing on only statistically significant information but being open to all significant information. It would also be helpful for us to help the medical professionals deal with less approachable and less controllable data, aiming to make them less willing to turn to be unethical as a form of escapism from those data. In addition, the author pointed out that being quantitative to all valuable information would be essential for statisticians in medical trials. ","80.39,75.60"
https://hbr.org/2018/08/what-data-scientists-really-do-according-to-35-data-scientists,"This article is about what data scientists do, according to data scientists. Firstly, data scientists perform data analytics based on their solid understanding of data and then achieve sustainable growth by applying several methods (experiments, statistical methods, etc.). After that,  they construct the machine-learning algorithm to better understand the data and even forecast so that the customers could make a better choice. Someone points out that, the skills that data scientists require are still evolving. Also, data scientists are required to be more specialized. The usual challenges among data scientists are to keep ethics while manipulating data and performing analysis.",,"82.68,"